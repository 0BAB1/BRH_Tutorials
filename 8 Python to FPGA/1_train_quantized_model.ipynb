{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71a8839-7c58-4862-a5f0-0f7e97085918",
   "metadata": {},
   "source": [
    "# THIS NOTEBOOK TRAINS A FASHION-MNIST MODEL USING 0-255 VALUES (UINT8)\n",
    "\n",
    "After creating the docker environement,\n",
    "\n",
    "The goal is to create a MNIST Fasion model in pytorch and experiment with the different parameters\n",
    "\n",
    "Then, we will do the same model but fully quantized and start adapting it for FINN\n",
    "\n",
    "> Side note : We'll use Quantization Aware Training (QAT) for this tutorial, but another possibility is to use Post-Training Quantization (PTQ) [Here is a resource to learn more but it's not really necessary for this tutorial](https://www.youtube.com/watch?v=0VdNflU08yA) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09c1a7-e82e-4d14-bced-29aaf340638c",
   "metadata": {},
   "source": [
    "## Base model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9dec1-299c-45ec-bc1a-41d4825fea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "user_name = \"rootmin\" # REPLACE THIS WITH YOUR HOST MACHINE USER NAME \n",
    "root_dir = f\"/tmp/finn_dev_{user_name}\"\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5934e06",
   "metadata": {},
   "source": [
    "Import the data and transform it, we don't normalize to really make it as simple as possible, the only transformation is to convert it to a pytorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9651dd1-25d9-4c48-8e02-08c28d29fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    #transforms.Normalize((0.5,), (0.5,))  # Normalize the tensor with mean and std\n",
    "]);\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',  # Directory to save the dataset\n",
    "    train=True,  # Load the training set\n",
    "    download=True,  # Download the dataset if it doesn't exist\n",
    "    transform=transform  # Apply the defined transformations\n",
    ");\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ba53e",
   "metadata": {},
   "source": [
    "Let's visualize the data a little bit... We see the data is ranging from 0 to 1 but we know our FPGA AI model IP will expect INTEGER values... More on that later !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb48f47-1369-4f87-8c9b-c0e29a8de5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset[5]\n",
    "image = np.array(image).squeeze()\n",
    "print(\"Min : \", np.min(image[0]), \" /// Max : \", np.max(image[0]))\n",
    "# plot the sample\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648a989-431b-418d-9dbf-118ca52aa7e1",
   "metadata": {},
   "source": [
    "# Actual QNN\n",
    "\n",
    "> QNN = Quantized Neural Network\n",
    "\n",
    "That's what we are about to do using Quantized Aware Training, and the best part is Brevitas handles all of it in the background !\n",
    "\n",
    "This part is about creating a quantized version of the model and adapting it to finn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dab5b0a0-9cfd-41c7-99cc-3c2d3365cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantLinear\n",
    "from brevitas.nn import QuantReLU\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "brevitas_input_size = 28 * 28\n",
    "brevitas_hidden1 = 64\n",
    "brevitas_hidden2 = 64\n",
    "brevitas_num_classes = 10\n",
    "weight_bit_width = 4\n",
    "act_bit_width = 4\n",
    "dropout_prob = 0.5\n",
    "\n",
    "#is this model fully quantized or only the wieghts, i shall dig to find out once done !\n",
    "brevitas_model = nn.Sequential(\n",
    "    QuantLinear(brevitas_input_size, brevitas_hidden1, bias=True, weight_bit_width=weight_bit_width),\n",
    "    nn.BatchNorm1d(brevitas_hidden1),\n",
    "    nn.Dropout(0.5),\n",
    "    QuantReLU(bit_width=act_bit_width),\n",
    "    QuantLinear(brevitas_hidden1, brevitas_hidden2, bias=True, weight_bit_width=weight_bit_width),\n",
    "    nn.BatchNorm1d(brevitas_hidden2),\n",
    "    nn.Dropout(0.5),\n",
    "    QuantReLU(bit_width=act_bit_width),\n",
    "    QuantLinear(brevitas_hidden2, brevitas_num_classes, bias=True, weight_bit_width=weight_bit_width),\n",
    "    QuantReLU(bit_width=act_bit_width)\n",
    ")\n",
    "\n",
    "# uncomment to check the network object\n",
    "# brevitas_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289eec74",
   "metadata": {},
   "source": [
    "### The input data has to be quantized.\n",
    "\n",
    "Normaly in brevistas, we can use the ```QuantIdentity()``` layer for this but unfortunatly, it does not convert to hardware (yet) in FINN.\n",
    "\n",
    "It its really not a problem, as we can just quantize the input data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "670acb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a custom quantization function\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    \n",
    "    return q_x\n",
    "\n",
    "# Define the quantized transform\n",
    "transform_quantized = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Lambda(lambda x: quantize_tensor(x))  # Apply quantization\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset_qnt = datasets.FashionMNIST(\n",
    "    root='./data',  # Directory to save the dataset\n",
    "    train=True,  # Load the training set\n",
    "    download=True,  # Download the dataset if it doesn't exist\n",
    "    transform=transform_quantized  # Apply the defined transformations\n",
    ");\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset_qnt = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform_quantized\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset_qnt, 100)\n",
    "test_loader = DataLoader(test_dataset_qnt, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84a0bd",
   "metadata": {},
   "source": [
    "Let's re-visualize the data now... That's better !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c819d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset_qnt[10]\n",
    "image = np.array(image).squeeze()\n",
    "print(\"Min : \", np.min(image), \" /// Max : \", np.max(image))\n",
    "print(image.dtype)\n",
    "# plot the sample\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a58259",
   "metadata": {},
   "source": [
    "## Actual training\n",
    "\n",
    "Now, just like in a regular PyTorch workflow, we train the model using a simple trainng loop.\n",
    "\n",
    "Note that it takes a bit of time because\n",
    "- We train on CPU\n",
    "- QAT \"simulates\" Quantization using quant/dequant layers (so the model becomes robust to quantization, which is not the case in PTQ) and backpropagation is different and that takes up computing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d85fe-986a-44f4-84d1-fce1d5591f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(brevitas_model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "num_epochs = 5\n",
    "brevitas_model.train()\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = brevitas_model(images.float()) # This just make the value a float ie 255 becomes 255,0 and not 1\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7ee17",
   "metadata": {},
   "source": [
    "We now test the model with ~85% accuracy, which is extremely close to non-quatized model performances (~85-86% when I tested it on my side when I made the course).\n",
    "\n",
    "This dataset isn't very complex though, this delta will depend on the data you have to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d239cf-edb2-4faa-a502-d05861a156fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "\n",
    "brevitas_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = brevitas_model(images.float())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(\"accuracy =\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c0b43",
   "metadata": {},
   "source": [
    "Here are some lines to execute the tensor that mmakes up the model and their different representations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3843919-1e83-4c50-accd-3e9292cecc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets have a quick look at the weights too\n",
    "print(brevitas_model[0].quant_weight())\n",
    "#internally, weoght are stored as float 32, here nare ways to visualize actual quantized weights :\n",
    "print(brevitas_model[0].quant_weight().int())\n",
    "print(brevitas_model[0].quant_weight().int().dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d9899-000b-4343-ac81-2c425b571519",
   "metadata": {},
   "source": [
    "# EXPORTING THE MODEL TO FINN-ONNX\n",
    "\n",
    "maybe you know ONNX, maybe you don't. At the end of the day, It's just a way to represent AI models (or just tensor operations) in an optimised an standard way. It also allows us to use Netron to visualize the model as a nice graph.\n",
    "\n",
    "> FINN-ONNX is just like ONNX, exept it's better for quantized models (under 8bit quantization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f27a3b-88c6-483e-be82-85d1ee53129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "filename = root_dir + \"/part1.onnx\"\n",
    "filename_clean = root_dir + \"/part1_clean.onnx\"\n",
    "\n",
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    min = 0\n",
    "    max = 2**num_bits - 1\n",
    "    \n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / max\n",
    "    zero_point = np.clip((-beta/scale),0,max).round().astype(np.int8)\n",
    "\n",
    "    quantized_arr = np.clip(np.round(arr / scale + zero_point), min, max).astype(np.float32)\n",
    "    \n",
    "    return quantized_arr\n",
    "\n",
    "#Crete a tensor ressembling the input tensor we saw earlier\n",
    "input_a = np.random.rand(1,28*28).astype(np.float32)\n",
    "input_a = asymmetric_quantize(input_a)\n",
    "print(np.max(input_a[0]))\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    brevitas_model, export_path=filename, input_t=input_t\n",
    ")\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(filename, out_file=filename_clean)\n",
    "\n",
    "# ModelWrapper\n",
    "model = ModelWrapper(filename_clean)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(root_dir + \"/ready_finn.onnx\")\n",
    "\n",
    "print(\"Model saved to \" + root_dir + \"/ready_finn.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7e045",
   "metadata": {},
   "source": [
    "## Visualization in Netron\n",
    "\n",
    "We can now visualise our network in netron, we can clearly identify each layer in this graph, Great !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f0250-5b1c-4276-9293-c29a1b112782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(root_dir + \"/ready_finn.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db41a1",
   "metadata": {},
   "source": [
    "We are now ready to move on to FINN !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
